{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "Case_Study - Supermaket_Campaign-1.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jkhan-prog/BasicPython/blob/main/CreditCard_Churn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBlKLGQ5GWlg"
      },
      "source": [
        "# Problem Statement:\n",
        "\n",
        "'All You Need' Supermarket is planning for the year end sale - they want to launch a new offer i.e. gold membership for only \\\\$499 that is of \\\\$999 on normal days(that gives 20% discount on all purchases) only for existing customers, for that they need to do a campaign through phone calls - best way to reduce the cost of campaign is to make a predictive model to classify customers who might purchase \n",
        "the offer, using the data they gathered during last year campaign.\n",
        "We will build a model for classifying whether customers will reply back with a positive response or not."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8v2sRm3AGWl6"
      },
      "source": [
        "# Objective:\n",
        "- What are the different factors which affect the target variable? What business recommendations can we give based on the analysis?\n",
        "- How can we improve model performance using hyperparameter tuning and prevent data leakage using pipelines while building a model to predict the response of a customer?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3X6xeAJGWl_"
      },
      "source": [
        "### Data Dictionary\n",
        "\n",
        "- Response (target) - 1 if customer accepted the offer in the last campaign, 0 otherwise\n",
        "- ID - Unique ID of each customer\n",
        "- Year_Birth - Age of the customer\n",
        "- Complain - 1 if customer complained in the last 2 years\n",
        "- Dt_Customer - date of customer's enrollment with the company\n",
        "- Education - customer's level of education\n",
        "- Marital - customer's marital status\n",
        "- Kidhome - number of small children in customer's household\n",
        "- Teenhome - number of teenagers in customer's household\n",
        "- Income - customer's yearly household income\n",
        "- MntFishProducts - amount spent on fish products in the last 2 years\n",
        "- MntMeatProducts - amount spent on meat products in the last 2 years\n",
        "- MntFruits - amount spent on fruits products in the last 2 years\n",
        "- MntSweetProducts - amount spent on sweet products in the last 2 years\n",
        "- MntWines - amount spent on wine products in the last 2 years\n",
        "- MntGoldProds - amount spent on gold products in the last 2 years\n",
        "- NumDealsPurchases - number of purchases made with discount\n",
        "- NumCatalogPurchases - number of purchases made using catalogue\n",
        "- NumStorePurchases - number of purchases made directly in stores\n",
        "- NumWebPurchases - number of purchases made through company's web site\n",
        "- NumWebVisitsMonth - number of visits to company's web site in the last month\n",
        "- Recency - number of days since the last purchase"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfNWG3daGWmH"
      },
      "source": [
        "## Import necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWwNuS4qGWmK"
      },
      "source": [
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "# Libraries to help with reading and manipulating data\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# libaries to help with data visualization\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Libraries to tune model, get different metric scores, and split data\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "\n",
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.pipeline import Pipeline, make_pipeline\n",
        "\n",
        "#libraries to help with model building\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import (\n",
        "    AdaBoostClassifier,\n",
        "    GradientBoostingClassifier,\n",
        "    RandomForestClassifier)\n",
        "from xgboost import XGBClassifier"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NmXv-gGZGWmP"
      },
      "source": [
        "## Load and view the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cR7L2-lzGWmT"
      },
      "source": [
        "data = pd.read_csv(\"BankChurners.csv\")"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        },
        "id": "FZtdOdpjGWmW",
        "outputId": "727767df-55e0-4835-ed4e-e6e5da0735c7"
      },
      "source": [
        "data.head()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>CLIENTNUM</th>\n",
              "      <th>Attrition_Flag</th>\n",
              "      <th>Customer_Age</th>\n",
              "      <th>Gender</th>\n",
              "      <th>Dependent_count</th>\n",
              "      <th>Education_Level</th>\n",
              "      <th>Marital_Status</th>\n",
              "      <th>Income_Category</th>\n",
              "      <th>Card_Category</th>\n",
              "      <th>Months_on_book</th>\n",
              "      <th>Total_Relationship_Count</th>\n",
              "      <th>Months_Inactive_12_mon</th>\n",
              "      <th>Contacts_Count_12_mon</th>\n",
              "      <th>Credit_Limit</th>\n",
              "      <th>Total_Revolving_Bal</th>\n",
              "      <th>Avg_Open_To_Buy</th>\n",
              "      <th>Total_Amt_Chng_Q4_Q1</th>\n",
              "      <th>Total_Trans_Amt</th>\n",
              "      <th>Total_Trans_Ct</th>\n",
              "      <th>Total_Ct_Chng_Q4_Q1</th>\n",
              "      <th>Avg_Utilization_Ratio</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>768805383</td>\n",
              "      <td>Existing Customer</td>\n",
              "      <td>45</td>\n",
              "      <td>M</td>\n",
              "      <td>3</td>\n",
              "      <td>High School</td>\n",
              "      <td>Married</td>\n",
              "      <td>$60K - $80K</td>\n",
              "      <td>Blue</td>\n",
              "      <td>39</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>12691.0</td>\n",
              "      <td>777</td>\n",
              "      <td>11914.0</td>\n",
              "      <td>1.335</td>\n",
              "      <td>1144</td>\n",
              "      <td>42</td>\n",
              "      <td>1.625</td>\n",
              "      <td>0.061</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>818770008</td>\n",
              "      <td>Existing Customer</td>\n",
              "      <td>49</td>\n",
              "      <td>F</td>\n",
              "      <td>5</td>\n",
              "      <td>Graduate</td>\n",
              "      <td>Single</td>\n",
              "      <td>Less than $40K</td>\n",
              "      <td>Blue</td>\n",
              "      <td>44</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>8256.0</td>\n",
              "      <td>864</td>\n",
              "      <td>7392.0</td>\n",
              "      <td>1.541</td>\n",
              "      <td>1291</td>\n",
              "      <td>33</td>\n",
              "      <td>3.714</td>\n",
              "      <td>0.105</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>713982108</td>\n",
              "      <td>Existing Customer</td>\n",
              "      <td>51</td>\n",
              "      <td>M</td>\n",
              "      <td>3</td>\n",
              "      <td>Graduate</td>\n",
              "      <td>Married</td>\n",
              "      <td>$80K - $120K</td>\n",
              "      <td>Blue</td>\n",
              "      <td>36</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3418.0</td>\n",
              "      <td>0</td>\n",
              "      <td>3418.0</td>\n",
              "      <td>2.594</td>\n",
              "      <td>1887</td>\n",
              "      <td>20</td>\n",
              "      <td>2.333</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>769911858</td>\n",
              "      <td>Existing Customer</td>\n",
              "      <td>40</td>\n",
              "      <td>F</td>\n",
              "      <td>4</td>\n",
              "      <td>High School</td>\n",
              "      <td>Unknown</td>\n",
              "      <td>Less than $40K</td>\n",
              "      <td>Blue</td>\n",
              "      <td>34</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>3313.0</td>\n",
              "      <td>2517</td>\n",
              "      <td>796.0</td>\n",
              "      <td>1.405</td>\n",
              "      <td>1171</td>\n",
              "      <td>20</td>\n",
              "      <td>2.333</td>\n",
              "      <td>0.760</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>709106358</td>\n",
              "      <td>Existing Customer</td>\n",
              "      <td>40</td>\n",
              "      <td>M</td>\n",
              "      <td>3</td>\n",
              "      <td>Uneducated</td>\n",
              "      <td>Married</td>\n",
              "      <td>$60K - $80K</td>\n",
              "      <td>Blue</td>\n",
              "      <td>21</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>4716.0</td>\n",
              "      <td>0</td>\n",
              "      <td>4716.0</td>\n",
              "      <td>2.175</td>\n",
              "      <td>816</td>\n",
              "      <td>28</td>\n",
              "      <td>2.500</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   CLIENTNUM     Attrition_Flag  ...  Total_Ct_Chng_Q4_Q1 Avg_Utilization_Ratio\n",
              "0  768805383  Existing Customer  ...                1.625                 0.061\n",
              "1  818770008  Existing Customer  ...                3.714                 0.105\n",
              "2  713982108  Existing Customer  ...                2.333                 0.000\n",
              "3  769911858  Existing Customer  ...                2.333                 0.760\n",
              "4  709106358  Existing Customer  ...                2.500                 0.000\n",
              "\n",
              "[5 rows x 21 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JAtkZYtnGWmd",
        "outputId": "855784f8-978d-4a5a-f6c9-8ba1661afbf3"
      },
      "source": [
        "data.info()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 10127 entries, 0 to 10126\n",
            "Data columns (total 21 columns):\n",
            " #   Column                    Non-Null Count  Dtype  \n",
            "---  ------                    --------------  -----  \n",
            " 0   CLIENTNUM                 10127 non-null  int64  \n",
            " 1   Attrition_Flag            10127 non-null  object \n",
            " 2   Customer_Age              10127 non-null  int64  \n",
            " 3   Gender                    10127 non-null  object \n",
            " 4   Dependent_count           10127 non-null  int64  \n",
            " 5   Education_Level           10127 non-null  object \n",
            " 6   Marital_Status            10127 non-null  object \n",
            " 7   Income_Category           10127 non-null  object \n",
            " 8   Card_Category             10127 non-null  object \n",
            " 9   Months_on_book            10127 non-null  int64  \n",
            " 10  Total_Relationship_Count  10127 non-null  int64  \n",
            " 11  Months_Inactive_12_mon    10127 non-null  int64  \n",
            " 12  Contacts_Count_12_mon     10127 non-null  int64  \n",
            " 13  Credit_Limit              10127 non-null  float64\n",
            " 14  Total_Revolving_Bal       10127 non-null  int64  \n",
            " 15  Avg_Open_To_Buy           10127 non-null  float64\n",
            " 16  Total_Amt_Chng_Q4_Q1      10127 non-null  float64\n",
            " 17  Total_Trans_Amt           10127 non-null  int64  \n",
            " 18  Total_Trans_Ct            10127 non-null  int64  \n",
            " 19  Total_Ct_Chng_Q4_Q1       10127 non-null  float64\n",
            " 20  Avg_Utilization_Ratio     10127 non-null  float64\n",
            "dtypes: float64(5), int64(10), object(6)\n",
            "memory usage: 1.6+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fiwHusS7GWmh"
      },
      "source": [
        "- There are total 20 columns and 10127 observations in the dataset\n",
        "- There are no missing values in any features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ehy6ETa8GWmk"
      },
      "source": [
        "**Let's check the number of unique values in each column**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oN4FSKZbGWmm",
        "outputId": "4c7c99cf-cc31-4726-d84b-ca603be6d91e"
      },
      "source": [
        "data.nunique()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CLIENTNUM                   10127\n",
              "Attrition_Flag                  2\n",
              "Customer_Age                   45\n",
              "Gender                          2\n",
              "Dependent_count                 6\n",
              "Education_Level                 7\n",
              "Marital_Status                  4\n",
              "Income_Category                 6\n",
              "Card_Category                   4\n",
              "Months_on_book                 44\n",
              "Total_Relationship_Count        6\n",
              "Months_Inactive_12_mon          7\n",
              "Contacts_Count_12_mon           7\n",
              "Credit_Limit                 6205\n",
              "Total_Revolving_Bal          1974\n",
              "Avg_Open_To_Buy              6813\n",
              "Total_Amt_Chng_Q4_Q1         1158\n",
              "Total_Trans_Amt              5033\n",
              "Total_Trans_Ct                126\n",
              "Total_Ct_Chng_Q4_Q1           830\n",
              "Avg_Utilization_Ratio         964\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vK3xS3w0GWmp"
      },
      "source": [
        "- We can drop the column - `CLIENTNUM' as it is unique for each customer and will not add value to the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zQ127UIULuc6",
        "outputId": "233298fe-776a-4428-e225-16e568e28db9"
      },
      "source": [
        "data.info()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 10127 entries, 0 to 10126\n",
            "Data columns (total 20 columns):\n",
            " #   Column                    Non-Null Count  Dtype  \n",
            "---  ------                    --------------  -----  \n",
            " 0   Attrition_Flag            10127 non-null  object \n",
            " 1   Customer_Age              10127 non-null  int64  \n",
            " 2   Gender                    10127 non-null  object \n",
            " 3   Dependent_count           10127 non-null  int64  \n",
            " 4   Education_Level           10127 non-null  object \n",
            " 5   Marital_Status            10127 non-null  object \n",
            " 6   Income_Category           10127 non-null  object \n",
            " 7   Card_Category             10127 non-null  object \n",
            " 8   Months_on_book            10127 non-null  int64  \n",
            " 9   Total_Relationship_Count  10127 non-null  int64  \n",
            " 10  Months_Inactive_12_mon    10127 non-null  int64  \n",
            " 11  Contacts_Count_12_mon     10127 non-null  int64  \n",
            " 12  Credit_Limit              10127 non-null  float64\n",
            " 13  Total_Revolving_Bal       10127 non-null  int64  \n",
            " 14  Avg_Open_To_Buy           10127 non-null  float64\n",
            " 15  Total_Amt_Chng_Q4_Q1      10127 non-null  float64\n",
            " 16  Total_Trans_Amt           10127 non-null  int64  \n",
            " 17  Total_Trans_Ct            10127 non-null  int64  \n",
            " 18  Total_Ct_Chng_Q4_Q1       10127 non-null  float64\n",
            " 19  Avg_Utilization_Ratio     10127 non-null  float64\n",
            "dtypes: float64(5), int64(9), object(6)\n",
            "memory usage: 1.5+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2fTKvBGiGWmr"
      },
      "source": [
        "# Dropping columns - CLIENTNUM\n",
        "data.drop(columns=[\"CLIENTNUM\"], inplace=True)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4RWGZRF8GWmt"
      },
      "source": [
        "**Summary of the data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 483
        },
        "id": "bZivM8dtGWmv",
        "outputId": "a5de9598-49b7-4eeb-9553-4c1c0def4d93"
      },
      "source": [
        "data.describe().T"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "      <th>mean</th>\n",
              "      <th>std</th>\n",
              "      <th>min</th>\n",
              "      <th>25%</th>\n",
              "      <th>50%</th>\n",
              "      <th>75%</th>\n",
              "      <th>max</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Customer_Age</th>\n",
              "      <td>10127.0</td>\n",
              "      <td>46.325960</td>\n",
              "      <td>8.016814</td>\n",
              "      <td>26.0</td>\n",
              "      <td>41.000</td>\n",
              "      <td>46.000</td>\n",
              "      <td>52.000</td>\n",
              "      <td>73.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Dependent_count</th>\n",
              "      <td>10127.0</td>\n",
              "      <td>2.346203</td>\n",
              "      <td>1.298908</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.000</td>\n",
              "      <td>2.000</td>\n",
              "      <td>3.000</td>\n",
              "      <td>5.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Months_on_book</th>\n",
              "      <td>10127.0</td>\n",
              "      <td>35.928409</td>\n",
              "      <td>7.986416</td>\n",
              "      <td>13.0</td>\n",
              "      <td>31.000</td>\n",
              "      <td>36.000</td>\n",
              "      <td>40.000</td>\n",
              "      <td>56.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Total_Relationship_Count</th>\n",
              "      <td>10127.0</td>\n",
              "      <td>3.812580</td>\n",
              "      <td>1.554408</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.000</td>\n",
              "      <td>4.000</td>\n",
              "      <td>5.000</td>\n",
              "      <td>6.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Months_Inactive_12_mon</th>\n",
              "      <td>10127.0</td>\n",
              "      <td>2.341167</td>\n",
              "      <td>1.010622</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.000</td>\n",
              "      <td>2.000</td>\n",
              "      <td>3.000</td>\n",
              "      <td>6.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Contacts_Count_12_mon</th>\n",
              "      <td>10127.0</td>\n",
              "      <td>2.455317</td>\n",
              "      <td>1.106225</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.000</td>\n",
              "      <td>2.000</td>\n",
              "      <td>3.000</td>\n",
              "      <td>6.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Credit_Limit</th>\n",
              "      <td>10127.0</td>\n",
              "      <td>8631.953698</td>\n",
              "      <td>9088.776650</td>\n",
              "      <td>1438.3</td>\n",
              "      <td>2555.000</td>\n",
              "      <td>4549.000</td>\n",
              "      <td>11067.500</td>\n",
              "      <td>34516.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Total_Revolving_Bal</th>\n",
              "      <td>10127.0</td>\n",
              "      <td>1162.814061</td>\n",
              "      <td>814.987335</td>\n",
              "      <td>0.0</td>\n",
              "      <td>359.000</td>\n",
              "      <td>1276.000</td>\n",
              "      <td>1784.000</td>\n",
              "      <td>2517.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Avg_Open_To_Buy</th>\n",
              "      <td>10127.0</td>\n",
              "      <td>7469.139637</td>\n",
              "      <td>9090.685324</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1324.500</td>\n",
              "      <td>3474.000</td>\n",
              "      <td>9859.000</td>\n",
              "      <td>34516.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Total_Amt_Chng_Q4_Q1</th>\n",
              "      <td>10127.0</td>\n",
              "      <td>0.759941</td>\n",
              "      <td>0.219207</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.631</td>\n",
              "      <td>0.736</td>\n",
              "      <td>0.859</td>\n",
              "      <td>3.397</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Total_Trans_Amt</th>\n",
              "      <td>10127.0</td>\n",
              "      <td>4404.086304</td>\n",
              "      <td>3397.129254</td>\n",
              "      <td>510.0</td>\n",
              "      <td>2155.500</td>\n",
              "      <td>3899.000</td>\n",
              "      <td>4741.000</td>\n",
              "      <td>18484.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Total_Trans_Ct</th>\n",
              "      <td>10127.0</td>\n",
              "      <td>64.858695</td>\n",
              "      <td>23.472570</td>\n",
              "      <td>10.0</td>\n",
              "      <td>45.000</td>\n",
              "      <td>67.000</td>\n",
              "      <td>81.000</td>\n",
              "      <td>139.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Total_Ct_Chng_Q4_Q1</th>\n",
              "      <td>10127.0</td>\n",
              "      <td>0.712222</td>\n",
              "      <td>0.238086</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.582</td>\n",
              "      <td>0.702</td>\n",
              "      <td>0.818</td>\n",
              "      <td>3.714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Avg_Utilization_Ratio</th>\n",
              "      <td>10127.0</td>\n",
              "      <td>0.274894</td>\n",
              "      <td>0.275691</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.023</td>\n",
              "      <td>0.176</td>\n",
              "      <td>0.503</td>\n",
              "      <td>0.999</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                            count         mean  ...        75%        max\n",
              "Customer_Age              10127.0    46.325960  ...     52.000     73.000\n",
              "Dependent_count           10127.0     2.346203  ...      3.000      5.000\n",
              "Months_on_book            10127.0    35.928409  ...     40.000     56.000\n",
              "Total_Relationship_Count  10127.0     3.812580  ...      5.000      6.000\n",
              "Months_Inactive_12_mon    10127.0     2.341167  ...      3.000      6.000\n",
              "Contacts_Count_12_mon     10127.0     2.455317  ...      3.000      6.000\n",
              "Credit_Limit              10127.0  8631.953698  ...  11067.500  34516.000\n",
              "Total_Revolving_Bal       10127.0  1162.814061  ...   1784.000   2517.000\n",
              "Avg_Open_To_Buy           10127.0  7469.139637  ...   9859.000  34516.000\n",
              "Total_Amt_Chng_Q4_Q1      10127.0     0.759941  ...      0.859      3.397\n",
              "Total_Trans_Amt           10127.0  4404.086304  ...   4741.000  18484.000\n",
              "Total_Trans_Ct            10127.0    64.858695  ...     81.000    139.000\n",
              "Total_Ct_Chng_Q4_Q1       10127.0     0.712222  ...      0.818      3.714\n",
              "Avg_Utilization_Ratio     10127.0     0.274894  ...      0.503      0.999\n",
              "\n",
              "[14 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXd7dHbUGWmy"
      },
      "source": [
        "- `Year_Birth` has a large range of values i.e. 1893 to 1996.\n",
        "- Columns - `MntFruits, MntWines, MntMeatProducts, MntFishProducts, MntSweetProducts` might have outliers on the right end as there is a large differene between between 75th percentile and maximum values.\n",
        "- Recency has an aprrox equal mean and median which is equal to 49.\n",
        "- Highest mean amount spent in the last two years is for wines (approx 304), followed by meat products (approx 167).\n",
        "- The distribution of classes in the `Response` variable is imbalanced as most of the values are 0."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUdAM8s0GWm1"
      },
      "source": [
        "## Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "av7226JOGWm4"
      },
      "source": [
        "**Adding age of the customers to the data using given birth years**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        },
        "id": "Ms4gbiZnGWm7",
        "outputId": "f7d4a72f-2a70-4caa-ca9b-4965ec64b3de"
      },
      "source": [
        "# To calculate age we'll subtract the year 2016 because variables account for the last 2 years\n",
        "# and we have customers registered till 2014 only\n",
        "# We need to convert strings values to dates first to use subraction\n",
        "data[\"Age\"] = 2016 - pd.to_datetime(data[\"Year_Birth\"], format=\"%Y\").apply(\n",
        "    lambda x: x.year\n",
        ")\n",
        "\n",
        "data[\"Age\"].sort_values()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2897\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2898\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2899\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'Year_Birth'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-8276d89f4435>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# and we have customers registered till 2014 only\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# We need to convert strings values to dates first to use subraction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m data[\"Age\"] = 2016 - pd.to_datetime(data[\"Year_Birth\"], format=\"%Y\").apply(\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myear\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m )\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2904\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2905\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2906\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2907\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2908\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2898\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2899\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2900\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2902\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'Year_Birth'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDZscwjsGWm-"
      },
      "source": [
        "- We can see that there are 3 observations with age greater than 100 i.e. 116, 117 and 123 which is highly unlikely to be true.\n",
        "- We can cap the value for age variables to the next highest value i.e. 76."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3mp_JOJIGWnB"
      },
      "source": [
        "# Capping age variable\n",
        "data[\"Age\"].clip(upper=76, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CGJZYOrGWnE"
      },
      "source": [
        "**Using Dt_Customer to add features to the data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BrrBPEpUGWnG"
      },
      "source": [
        "# The feature Dt_Customer represents dates of customerâ€™s enrollment with the company.\n",
        "# Let's convert this to datetime format\n",
        "data[\"Dt_Customer\"] = pd.to_datetime(data[\"Dt_Customer\"]) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nKY5hw-fGWnK"
      },
      "source": [
        "# Extracting registration year from the date\n",
        "data[\"Reg_year\"] = data[\"Dt_Customer\"].apply(lambda x: x.year)\n",
        "\n",
        "# Extracting registration quarter from the date\n",
        "data[\"Reg_quarter\"] = data[\"Dt_Customer\"].apply(lambda x: x.quarter)\n",
        "\n",
        "# Extracting registration month from the date\n",
        "data[\"Reg_month\"] = data[\"Dt_Customer\"].apply(lambda x: x.month)\n",
        "\n",
        "# Extracting registration week from the date\n",
        "data[\"Reg_week\"] = data[\"Dt_Customer\"].apply(lambda x: x.day // 7)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qH8v_7G4GWnN"
      },
      "source": [
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDZEeMu_GWna"
      },
      "source": [
        "**Let's check the count of each unique category in each of the categorical variables.** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7BAsVxf7GWnc"
      },
      "source": [
        "# Making a list of all categorical variables\n",
        "cat_col = [\n",
        "    \"Education\",\n",
        "    \"Marital_Status\",\n",
        "    \"Kidhome\",\n",
        "    \"Teenhome\",\n",
        "    \"Complain\",\n",
        "    \"Response\",\n",
        "    \"Reg_year\",\n",
        "    \"Reg_quarter\",\n",
        "    \"Reg_month\",\n",
        "    \"Reg_week\",\n",
        "]\n",
        "\n",
        "# Printing number of count of each unique value in each column\n",
        "for column in cat_col:\n",
        "    print(data[column].value_counts())\n",
        "    print(\"-\" * 40)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99zzwzSTGWng"
      },
      "source": [
        "- In education, 2n cycle and Master means the same thing. We can combine these two categories.\n",
        "- There are many categories in martial status. We can combine categories 'Alone', 'Absurd' and 'YOLO' with 'Single' and 'Together' category with 'Married'.\n",
        "- There are only 21 customers who complained in the last two years.\n",
        "- We have 1906 observation for the 0 class but only 334 observations for the class 1.\n",
        "- There are only three years in the customer registration data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FosMSE-bGWnj"
      },
      "source": [
        "# Replacing 2n Cycle with Master\n",
        "data[\"Education\"] = data[\"Education\"].replace(\"2n Cycle\", \"Master\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fqZi9RkCGWnl"
      },
      "source": [
        "# Replacing YOLO, Alone, Absurd with single and Together with Married\n",
        "data[\"Marital_Status\"] = data[\"Marital_Status\"].replace(\n",
        "    [\"YOLO\", \"Alone\", \"Absurd\"], \"Single\"\n",
        ")\n",
        "data[\"Marital_Status\"] = data[\"Marital_Status\"].replace([\"Together\"], \"Married\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0z6RKzjPGWnn"
      },
      "source": [
        "**Imputing missing values in income column**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4R-q6ZMrGWno"
      },
      "source": [
        "# number of missing values in each column\n",
        "data.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s54c1_qxGWnr"
      },
      "source": [
        "# Percentage of missing values in income column\n",
        "round(data.isna().sum() / data.isna().count() * 100, 2)[\"Income\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AknP2-LsGWnt"
      },
      "source": [
        "**We can add a column - total amount spent by each customer in the last 2 years**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OMWfvfnHGWnv"
      },
      "source": [
        "data[\"Total_Amount_Spent\"] = data[\n",
        "    [\n",
        "        \"MntWines\",\n",
        "        \"MntFruits\",\n",
        "        \"MntMeatProducts\",\n",
        "        \"MntFishProducts\",\n",
        "        \"MntSweetProducts\",\n",
        "        \"MntGoldProds\",\n",
        "    ]\n",
        "].sum(axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eszWkAHGWny"
      },
      "source": [
        "## EDA\n",
        "\n",
        "### Univariate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "665B-FNBGWn0"
      },
      "source": [
        "# While doing uni-variate analysis of numerical variables we want to study their central tendency\n",
        "# and dispersion.\n",
        "# Let us write a function that will help us create boxplot and histogram for any input numerical\n",
        "# variable.\n",
        "# This function takes the numerical column as the input and returns the boxplots\n",
        "# and histograms for the variable.\n",
        "# Let us see if this help us write faster and cleaner code.\n",
        "def histogram_boxplot(feature, figsize=(15, 10), bins=None):\n",
        "    \"\"\"Boxplot and histogram combined\n",
        "    feature: 1-d feature array\n",
        "    figsize: size of fig (default (9,8))\n",
        "    bins: number of bins (default None / auto)\n",
        "    \"\"\"\n",
        "    f2, (ax_box2, ax_hist2) = plt.subplots(\n",
        "        nrows=2,  # Number of rows of the subplot grid= 2\n",
        "        sharex=True,  # x-axis will be shared among all subplots\n",
        "        gridspec_kw={\"height_ratios\": (0.25, 0.75)},\n",
        "        figsize=figsize,\n",
        "    )  # creating the 2 subplots\n",
        "    sns.boxplot(\n",
        "        feature, ax=ax_box2, showmeans=True, color=\"violet\"\n",
        "    )  # boxplot will be created and a star will indicate the mean value of the column\n",
        "    sns.distplot(\n",
        "        feature, kde=F, ax=ax_hist2, bins=bins, palette=\"winter\"\n",
        "    ) if bins else sns.distplot(\n",
        "        feature, kde=False, ax=ax_hist2\n",
        "    )  # For histogram\n",
        "    ax_hist2.axvline(\n",
        "        np.mean(feature), color=\"green\", linestyle=\"--\"\n",
        "    )  # Add mean to the histogram\n",
        "    ax_hist2.axvline(\n",
        "        np.median(feature), color=\"black\", linestyle=\"-\"\n",
        "    )  # Add median to the histogram"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bMIuOVfiGWn3"
      },
      "source": [
        "# Observations on Customer_age\n",
        "histogram_boxplot(data[\"Age\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmZt3dQfGWn5"
      },
      "source": [
        "- As per the boxplot, there are no outliers in 'Age' variable\n",
        "- Age has a fairly normal distribution distribution with approx equal mean and median"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSryx6ryGWn7"
      },
      "source": [
        "# observations on Income\n",
        "histogram_boxplot(data[\"Income\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEApp_UGGWn-"
      },
      "source": [
        "- We can see there are some outliers in the income variable.\n",
        "- Some variation is always expected in real world scenarios for the income variable but we can remove the data point on extreme right end of the boxplot as it can be a data entry error."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6tYSOg2GWoA"
      },
      "source": [
        "# Dropping observaion with income greater than 20000. There is just 1 such observation\n",
        "data.drop(index=data[data.Income > 200000].index, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qxn2sYJ_GWoC"
      },
      "source": [
        "# observations on Recency\n",
        "histogram_boxplot(data[\"Recency\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqZV0g8FGWoF"
      },
      "source": [
        "- There are no outliers in 'Recency' variable\n",
        "- The distribution is fairly symmetric and uniformly distributed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbecCMQqGWoG"
      },
      "source": [
        "# observations on MntWines\n",
        "histogram_boxplot(data[\"MntWines\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7FgOEJ-GWoJ"
      },
      "source": [
        "- The distribution for amount spent on wines is highly skewed to the right\n",
        "- As median of the distribution is less than 200, more than 50% of customers have spent less than 200 on wines.\n",
        "- There are some outliers on the right end of the boxplot but we will not treat them as some variation is always expected in real world scenarios for variables like amount spent."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkozzTTPGWoL"
      },
      "source": [
        "# observations on MntFruits\n",
        "histogram_boxplot(data[\"MntFruits\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4G-CUKVGWoP"
      },
      "source": [
        "- The distribution for amount spent on fruits is highly skewed to the right.\n",
        "- As median of the distribution is less than 20, more than 50% of customers have spent less than 20 on fruits.\n",
        "- There are some outliers on the right end of the boxplot but we will not treat them as some variation is always expected in real world scenarios for variables like amount spent."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hm5P1_XXGWoR"
      },
      "source": [
        "# observations on MntMeatProducts\n",
        "histogram_boxplot(data[\"MntMeatProducts\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PIgY7uFIGWoV"
      },
      "source": [
        "- The distribution for amount spent on meat products is highly skewed to the right.\n",
        "- We can see that there are some extreme observations in the variable which can be considered as outliers as they very far from the rest of the values. \n",
        "- We can cap the value of the variable to the next highest value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtxPGSNwGWoX"
      },
      "source": [
        "# Checking 5 largest values of amount spend on meat products\n",
        "data.MntMeatProducts.nlargest(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0UP8OFnGWob"
      },
      "source": [
        "# Capping values for amount spent on meat products at next highest value i.e. 984\n",
        "data[\"MntMeatProducts\"].clip(upper=984, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lp4FXMAGGWod"
      },
      "source": [
        "# observations on MntFishProducts\n",
        "histogram_boxplot(data[\"MntFishProducts\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWGOX3KpGWog"
      },
      "source": [
        "- The distribution for amount spent on fish products is right skewed\n",
        "- There are some outliers on the right end in the boxplot but we will not treat them as this represents real market trend that some customers spend more on fish products than others."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Jpd_UR7GWoi"
      },
      "source": [
        "# observations on MntSweetProducts\n",
        "histogram_boxplot(data[\"MntSweetProducts\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5DNI6ns4GWol"
      },
      "source": [
        "- The distribution for the amount spent on sweet products is right skewed \n",
        "- There is one observation to the right extreme which can be considered as an outlier. \n",
        "- We will not remove all such data point as they represent real market trend but we can cap some of the extreme values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94z-e6p3GWoo"
      },
      "source": [
        "# Capping values for amount spent on sweet products at 198\n",
        "data[\"MntSweetProducts\"].clip(upper=198, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cLu5uBUsGWoq"
      },
      "source": [
        "# observations on MntGoldProds\n",
        "histogram_boxplot(data[\"MntGoldProds\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wi8KxnVNGWot"
      },
      "source": [
        "- The distribution for the amount spent on gold products is right skewed \n",
        "- There are some outliers in amount spent on gold products. We will not remove all such data point as they represent real market trend but we can cap some of the extreme values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t6-ujlKWGWow"
      },
      "source": [
        "# Capping values for amount spent on gold products at 250\n",
        "data[\"MntGoldProds\"].clip(upper=250, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nL-2K5jjGWoz"
      },
      "source": [
        "# observations on NumDealsPurchases\n",
        "histogram_boxplot(data[\"NumDealsPurchases\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkMf8OWMGWo2"
      },
      "source": [
        "- Majority of the customers have 2 or less than 2 deal purchases. \n",
        "- We can see that there some extreme observations in the variable. This represents the real market trend."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9KDLVpeIGWo3"
      },
      "source": [
        "# observations on NumWebPurchases\n",
        "histogram_boxplot(data[\"NumWebPurchases\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZCfMhCIGWo6"
      },
      "source": [
        "- The median of the distribution is 4 i.e. 50% customers have 4 or less than 4 web purchases. \n",
        "- We can see that there are some extreme observations in the variable. We can cap these values to the next highest number of purchases."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RN7uKEtvGWo8"
      },
      "source": [
        "# Capping values for number of web purchases at 11\n",
        "data[\"NumWebPurchases\"].clip(upper=11, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_bsfWNUGWo-"
      },
      "source": [
        "# observations on NumCatalogPurchases\n",
        "histogram_boxplot(data[\"NumCatalogPurchases\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVx2mrCwGWpA"
      },
      "source": [
        "- The most number of observations are for 0 catalog purchases.\n",
        "- The median of the distribution is 2 i.e. 50% customers have 2 or less than 2 catalog purchases. \n",
        "- We can see that there are two extreme observation in the variable. We can cap these values to the next highest number of purchases."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WjbOtsSeGWpB"
      },
      "source": [
        "# Capping values for number of catalog purchases at 11\n",
        "data[\"NumCatalogPurchases\"].clip(upper=11, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGkegCHgGWpD"
      },
      "source": [
        "# observations on NumStorePurchases\n",
        "histogram_boxplot(data[\"NumStorePurchases\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvUKNaC6GWpF"
      },
      "source": [
        "- There are very few observations with less than 2 purchases from the store\n",
        "- Most of the customers have 4 or 5 purchases from the store\n",
        "- There are no outliers in this variable"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "186pYidxGWpH"
      },
      "source": [
        "# observations on NumWebVisitsMonth\n",
        "histogram_boxplot(data[\"NumWebVisitsMonth\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uB9MCOuSGWpK"
      },
      "source": [
        "- The distribution for number of visits in a month is skewed and have some outliers at the right end.\n",
        "- We will not treat this as this represents general market trend"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "GMhXOR8AGWpL"
      },
      "source": [
        "def perc_on_bar(feature):\n",
        "    \"\"\"\n",
        "    plot\n",
        "    feature: categorical feature\n",
        "    the function won't work if a column is passed in hue parameter\n",
        "    \"\"\"\n",
        "    # Creating a countplot for the feature\n",
        "    sns.set(rc={\"figure.figsize\": (10, 5)})\n",
        "    ax = sns.countplot(x=feature, data=data)\n",
        "\n",
        "    total = len(feature)  # length of the column\n",
        "    for p in ax.patches:\n",
        "        percentage = \"{:.1f}%\".format(\n",
        "            100 * p.get_height() / total\n",
        "        )  # percentage of each class of the category\n",
        "        x = p.get_x() + p.get_width() / 2 - 0.1  # width of the plot\n",
        "        y = p.get_y() + p.get_height()  # hieght of the plot\n",
        "        ax.annotate(percentage, (x, y), size=14)  # annotate the percantage\n",
        "\n",
        "    plt.show()  # show the plot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pKozZ39zGWpN"
      },
      "source": [
        "# observations on Marital_Status\n",
        "perc_on_bar(data[\"Marital_Status\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ry1AMKPhGWpP"
      },
      "source": [
        "- Majority of the customers are married comprising approx 64% of total customers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IIWncMPsGWpQ"
      },
      "source": [
        "# observations on Education\n",
        "perc_on_bar(data[\"Education\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAr9AbSaGWpT"
      },
      "source": [
        "- Education of approx 50% customers is at graduation level.\n",
        "- Very few observations i.e. ~2% for customers with basic level education"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36oVNdCeGWpU"
      },
      "source": [
        "# observations on Kidhome\n",
        "perc_on_bar(data[\"Kidhome\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6F6j7-NBGWpW"
      },
      "source": [
        "- ~40% customers have 1 kid and ~58% customers have no kids at home\n",
        "- There are very few customers, approx 2%, with number of kids greater than 1 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LCb4yY_5GWpY"
      },
      "source": [
        "# observations on Teenhome\n",
        "perc_on_bar(data[\"Teenhome\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfvVO-HBGWpb"
      },
      "source": [
        "- Majority of the customers i.e. ~52% customers have no teen at home\n",
        "- There are very few customers, only ~2%, with number of teens greater than 1 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4swt3fUAGWpd"
      },
      "source": [
        "# observations on Complain\n",
        "perc_on_bar(data[\"Complain\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQM9_55IGWpf"
      },
      "source": [
        "- Approx 99% customers had no complaint in the last 2 years. This might be because the company provides good services or might be due to the lack of feedback options for customers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nah2jOwOGWpg"
      },
      "source": [
        "# observations on Registration year\n",
        "perc_on_bar(data[\"Reg_year\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvWn9W6tGWpi"
      },
      "source": [
        "- The number of customers registered are highest in the year 2013."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHGYDWYfGWpk"
      },
      "source": [
        "# observations on Registration quarter\n",
        "perc_on_bar(data[\"Reg_quarter\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qF110aRNGWpm"
      },
      "source": [
        "- There is no significant difference in the number of registrations for each quarter.\n",
        "- The number of registrations are slightly higher for 1st and the 4th quarter. This can be due to the festival season in these months.\n",
        "- Let's explore this further by plotting count of registration per month."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WR4wLZ5OGWpo"
      },
      "source": [
        "# observations on Registration month\n",
        "perc_on_bar(data[\"Reg_month\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2kK534LOGWpq"
      },
      "source": [
        "- This shows that the highest number of registration are in the months of winters i.e. March, May, August, October.\n",
        "- There is approx 3% reduction in the number of registrations from the month June to the month of July."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5aHRDM8GGWps"
      },
      "source": [
        "# observations on Registration week\n",
        "perc_on_bar(data[\"Reg_week\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHW4A8GAGWpv"
      },
      "source": [
        "- This shows that number of registrations decline at the end of the month i.e. in last two weeks.\n",
        "- This can be due to the fact that most people get salaries on the last day or first day of the month."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LI-t-w3vGWpw"
      },
      "source": [
        "# observations on Response\n",
        "perc_on_bar(data[\"Response\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vb-KgYzPGWpz"
      },
      "source": [
        "- Approx 85% customer's response was NO in the last campaign.\n",
        "- This shows that the distribution of classes in the target variable is imbalanced. We have only ~15% observations where response is YES."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6GP2TiRGWp1"
      },
      "source": [
        "### Bivariate Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_eRxud6GWp3"
      },
      "source": [
        "sns.pairplot(data, hue=\"Response\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3c_AHbAeGWp5"
      },
      "source": [
        "- There are overlaps i.e. no clear distinction in the distribution of variables for people who have taken the product and did not take the product.\n",
        "- Let's explore this further with the help of other plots."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9v6mDonGWp7"
      },
      "source": [
        "sns.set(rc={\"figure.figsize\": (10, 7)})\n",
        "sns.boxplot(y=\"Total_Amount_Spent\", x=\"Marital_Status\", data=data, orient=\"vertical\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFoKOSWEGWp9"
      },
      "source": [
        "- We can see that total amount spent is higher for widowed customers.\n",
        "- No significant difference in the amount spent by single, married or divorced customers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IyStZd6hGWp-"
      },
      "source": [
        "sns.boxplot(y=\"Total_Amount_Spent\", x=\"Education\", data=data, orient=\"vertical\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVjbTtx5GWqB"
      },
      "source": [
        "- As expected, the amount spent increases with the increase in education level.\n",
        "- Customers with graduation level education spend slightly more than the customers with master level education. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "idSq-2aQGWqC"
      },
      "source": [
        "pd.pivot_table(\n",
        "    data=data,\n",
        "    index=[\"Reg_year\", \"Reg_month\"],\n",
        "    values=\"Total_Amount_Spent\",\n",
        "    aggfunc=np.sum,\n",
        ").plot(kind=\"line\", marker=\"o\", linewidth=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WehQWh8MGWqF"
      },
      "source": [
        "- The plot clearly shows that the total amount spent has declined over the years.\n",
        "- The plot shows highest increase in the amount spent from the month of August to September 2012. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uh050K5yGWqH"
      },
      "source": [
        "sns.regplot(y=data.Total_Amount_Spent, x=data.Income)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZRf640KGGWqJ"
      },
      "source": [
        "- We can see that income and the total amount spent have a positive correlation.\n",
        "- The total amount spent is not much different for customers with income in the range of 20K to 60K but the difference is significant for customers in the range of 60K to 100K."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJRjtHxMGWqL"
      },
      "source": [
        "cols = data[\n",
        "    [\n",
        "        \"MntWines\",\n",
        "        \"MntGoldProds\",\n",
        "        \"MntMeatProducts\",\n",
        "        \"MntFruits\",\n",
        "        \"MntFishProducts\",\n",
        "        \"MntSweetProducts\",\n",
        "    ]\n",
        "].columns.tolist()\n",
        "plt.figure(figsize=(10, 10))\n",
        "\n",
        "for i, variable in enumerate(cols):\n",
        "    plt.subplot(3, 2, i + 1)\n",
        "    sns.boxplot(data[\"Response\"], data[variable])\n",
        "    plt.tight_layout()\n",
        "    plt.title(variable)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "183Vv7XOGWqO"
      },
      "source": [
        "- Each plot shows that customer spending more on any product is more likely to take the offer. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4PoWk3TJGWqQ"
      },
      "source": [
        "cols = data[[\"Recency\", \"Age\", \"Income\", \"Total_Amount_Spent\"]].columns.tolist()\n",
        "plt.figure(figsize=(10, 10))\n",
        "\n",
        "for i, variable in enumerate(cols):\n",
        "    plt.subplot(3, 2, i + 1)\n",
        "    sns.boxplot(data[\"Response\"], data[variable])\n",
        "    plt.tight_layout()\n",
        "    plt.title(variable)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9D2OWyHPGWqT"
      },
      "source": [
        "- Customers with lower recency i.e. less number of days since the last purchase, are more likely to take the offer.\n",
        "- Response does not depends much on the age.\n",
        "- Customers with higher income are more likely to take the offer.\n",
        "- Customers who spent more in the last 2 years are more likely to take the offer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_Ed2V7VGWqU"
      },
      "source": [
        "### Function to plot stacked bar charts for categorical columns\n",
        "def stacked_plot(x):\n",
        "    sns.set(palette=\"nipy_spectral\")\n",
        "    tab1 = pd.crosstab(x, data[\"Response\"], margins=True)\n",
        "    print(tab1)\n",
        "    print(\"-\" * 120)\n",
        "    tab = pd.crosstab(x, data[\"Response\"], normalize=\"index\")\n",
        "    tab.plot(kind=\"bar\", stacked=True, figsize=(10, 5))\n",
        "    plt.legend(loc=\"lower left\", frameon=False)\n",
        "    plt.legend(loc=\"upper left\", bbox_to_anchor=(1, 1))\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "uEhcRSLFGWqX"
      },
      "source": [
        "stacked_plot(data[\"Education\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-qXiqWGwGWqZ"
      },
      "source": [
        "- We can see a clear trend here that customers with higher education are more likely to take the offer.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tb7H89RkGWqb"
      },
      "source": [
        "stacked_plot(data[\"Marital_Status\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYI6-JYGGWqe"
      },
      "source": [
        "- We saw earlier that number of married customers are much more than single or divorced but divorced/widow customers are more likely to take the offer.\n",
        "- Single customers are more likely to take the offer than married customers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lzXIriDRGWqf"
      },
      "source": [
        "stacked_plot(data[\"Kidhome\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LA9bdBPeGWqi"
      },
      "source": [
        "- We can see that as number of kids increases, chances of customers taking the offer decreases.\n",
        "- Customers with no kids at home are more likely to take the offer which can be expected as this includes single customers as well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iKu5rZDWGWqk"
      },
      "source": [
        "stacked_plot(data[\"Teenhome\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8kiDuGp-GWqm"
      },
      "source": [
        "- Customers with no teens at home are most likely to take the offer.\n",
        "- Customers with two teens are more likely to take the offer than customers with 1 teenager."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mg9MrLLqGWqo"
      },
      "source": [
        "stacked_plot(data[\"Reg_year\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blgvLQGOGWqq"
      },
      "source": [
        "- Number of customers taking the offer is decreasing each subsequent year.\n",
        "- Let's explore this further for month wise distribution for each of the year."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRcEunG_GWqr"
      },
      "source": [
        "sns.set(rc={\"figure.figsize\": (15, 15)})\n",
        "sns.heatmap(\n",
        "    data.corr(),\n",
        "    annot=True,\n",
        "    linewidths=0.5,\n",
        "    center=0,\n",
        "    cbar=False,\n",
        "    cmap=\"YlGnBu\",\n",
        "    fmt=\"0.2f\",\n",
        ")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZ3bLIxXGWqu"
      },
      "source": [
        "- As expected, age and year birth have high negative correlation. We can drop one of them.\n",
        "- Registration month, quarter and year columns are highly correlated which can be expected as we extracted these columns from the same column.\n",
        "- We can drop one of the columns in quarter or month as they are almost perfectly correlated.\n",
        "- Total amount spent is correlated with variables they are associate with. We can drop this column.\n",
        "- Number of purchases is positively correlated with income which can be expected as customers with higher income might spend more than customers with lower income."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XwJGily0GWqv"
      },
      "source": [
        "## Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S4kDJRngGWqx"
      },
      "source": [
        "education = {'Basic':1, 'Graduation':2, 'Master':3, 'PhD':4}\n",
        "data['Education']=data['Education'].map(education)\n",
        "marital_status = {'Married':1,'Single':2, 'Divorced':3, 'Widow':4}\n",
        "data['Marital_Status']=data['Marital_Status'].map(marital_status)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x999Pn1FGWqz"
      },
      "source": [
        "**Split the data into train and test sets** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQJfGMC9GWq1"
      },
      "source": [
        "# Separating target variable and other variables\n",
        "X = data.drop(columns=\"Response\")\n",
        "Y = data[\"Response\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBD3pSxKGWq3"
      },
      "source": [
        "# Dropping birth year and Dt_Customer columns\n",
        "X.drop(\n",
        "    columns=[\n",
        "        \"Year_Birth\",\n",
        "        \"Dt_Customer\",\n",
        "        \"Reg_quarter\",\n",
        "        \"Total_Amount_Spent\",\n",
        "    ],\n",
        "    inplace=True,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "208kP3B4GWq4"
      },
      "source": [
        "# Splitting the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, Y, test_size=0.30, random_state=1, stratify=Y\n",
        ")\n",
        "print(X_train.shape, X_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pElH-TERGWq6"
      },
      "source": [
        "### Missing-Value Treatment\n",
        "\n",
        "* We will use KNN imputer to impute missing values.\n",
        "* `KNNImputer`: Each sample's missing values are imputed by looking at the n_neighbors nearest neighbors found in the training set. Default value for n_neighbors=5.\n",
        "* KNN imputer replaces missing values using the average of k nearest non-missing feature values.\n",
        "* Nearest points are found based on euclidean distance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUKhTy6VGWq8"
      },
      "source": [
        "imputer = KNNImputer(n_neighbors=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RqyVktxGGWq9"
      },
      "source": [
        "#Fit and transform the train data\n",
        "X_train=pd.DataFrame(imputer.fit_transform(X_train),columns=X_train.columns)\n",
        "\n",
        "#Transform the test data \n",
        "X_test=pd.DataFrame(imputer.transform(X_test),columns=X_test.columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MU07pTC8GWq_"
      },
      "source": [
        "#Checking that no column has missing values in train or test sets\n",
        "print(X_train.isna().sum())\n",
        "print('-'*30)\n",
        "print(X_test.isna().sum())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_p7BarqYGWrB"
      },
      "source": [
        "* All missing values have been treated.\n",
        "* Let's inverse map the encoded values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z9DeMFJdGWrD"
      },
      "source": [
        "## Function to inverse the encoding\n",
        "def inverse_mapping(x,y):\n",
        "    inv_dict = {v: k for k, v in x.items()}\n",
        "    X_train[y] = np.round(X_train[y]).map(inv_dict).astype('category')\n",
        "    X_test[y] = np.round(X_test[y]).map(inv_dict).astype('category')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5m4bfopoGWrE"
      },
      "source": [
        "inverse_mapping(education,'Education')\n",
        "inverse_mapping(marital_status,'Marital_Status')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-VmiKXOhGWrG"
      },
      "source": [
        "* Checking inverse mapped values/categories."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pq90Xu9MGWrH"
      },
      "source": [
        "cols = X_train.select_dtypes(include=['object','category'])\n",
        "for i in cols.columns:\n",
        "    print(X_train[i].value_counts())\n",
        "    print('*'*30)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ktX_80QnGWrK"
      },
      "source": [
        "cols = X_test.select_dtypes(include=['object','category'])\n",
        "for i in cols.columns:\n",
        "    print(X_train[i].value_counts())\n",
        "    print('*'*30)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ob3OgEDXGWrM"
      },
      "source": [
        "* Inverse mapping returned original labels."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bgDAq-dGWrO"
      },
      "source": [
        "### Encoding categorical varaibles"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRvSIklkGWrP"
      },
      "source": [
        "X_train=pd.get_dummies(X_train,drop_first=True)\n",
        "X_test=pd.get_dummies(X_test,drop_first=True)\n",
        "print(X_train.shape, X_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17sIgDBXGWrR"
      },
      "source": [
        "* After encoding there are 26 columns."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6c4Mw5nGWrT"
      },
      "source": [
        "## Building the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYZeor3RGWrV"
      },
      "source": [
        "### Model evaluation criterion:\n",
        "\n",
        "#### Model can make wrong predictions as:\n",
        "1. Predicting a customer will buy the product and the customer doesn't buy - Loss of resources\n",
        "2. Predicting a customer will not buy the product and the customer buys - Loss of opportunity\n",
        "\n",
        "#### Which case is more important? \n",
        "* Predicting that customer will not buy the product but he buys i.e. losing on a potential source of income for the company because that customer will not targeted by the marketing team when he should be targeted.\n",
        "\n",
        "#### How to reduce this loss i.e need to reduce False Negatives?\n",
        "* Company wants Recall to be maximized, greater the Recall lesser the chances of false negatives."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "il47qo6vGWrW"
      },
      "source": [
        "**Let's start by building different models using KFold and cross_val_score with pipelines and tune the best model using GridSearchCV and RandomizedSearchCV**\n",
        "\n",
        "- `Stratified K-Folds cross-validator` provides dataset indices to split data in train/validation sets. Split dataset into k consecutive folds (without shuffling by default) keeping distribution of both classes in each fold same as the target variable. Each fold is then used once as a validation while the k - 1 remaining folds form the training set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OPOyqKQqGWrY"
      },
      "source": [
        "models = []  # Empty list to store all the models\n",
        "\n",
        "# Appending pipelines for each model into the list\n",
        "models.append(\n",
        "    (\n",
        "        \"LR\",\n",
        "        Pipeline(\n",
        "            steps=[\n",
        "                (\"scaler\", StandardScaler()),\n",
        "                (\"log_reg\", LogisticRegression(random_state=1)),\n",
        "            ]\n",
        "        ),\n",
        "    )\n",
        ")\n",
        "models.append(\n",
        "    (\n",
        "        \"RF\",\n",
        "        Pipeline(\n",
        "            steps=[\n",
        "                (\"scaler\", StandardScaler()),\n",
        "                (\"random_forest\", RandomForestClassifier(random_state=1)),\n",
        "            ]\n",
        "        ),\n",
        "    )\n",
        ")\n",
        "models.append(\n",
        "    (\n",
        "        \"GBM\",\n",
        "        Pipeline(\n",
        "            steps=[\n",
        "                (\"scaler\", StandardScaler()),\n",
        "                (\"gradient_boosting\", GradientBoostingClassifier(random_state=1)),\n",
        "            ]\n",
        "        ),\n",
        "    )\n",
        ")\n",
        "models.append(\n",
        "    (\n",
        "        \"ADB\",\n",
        "        Pipeline(\n",
        "            steps=[\n",
        "                (\"scaler\", StandardScaler()),\n",
        "                (\"adaboost\", AdaBoostClassifier(random_state=1)),\n",
        "            ]\n",
        "        ),\n",
        "    )\n",
        ")\n",
        "models.append(\n",
        "    (\n",
        "        \"XGB\",\n",
        "        Pipeline(\n",
        "            steps=[\n",
        "                (\"scaler\", StandardScaler()),\n",
        "                (\"xgboost\", XGBClassifier(random_state=1,eval_metric='logloss')),\n",
        "            ]\n",
        "        ),\n",
        "    )\n",
        ")\n",
        "models.append(\n",
        "    (\n",
        "        \"DTREE\",\n",
        "        Pipeline(\n",
        "            steps=[\n",
        "                (\"scaler\", StandardScaler()),\n",
        "                (\"decision_tree\", DecisionTreeClassifier(random_state=1)),\n",
        "            ]\n",
        "        ),\n",
        "    )\n",
        ")\n",
        "\n",
        "results = []  # Empty list to store all model's CV scores\n",
        "names = []  # Empty list to store name of the models\n",
        "\n",
        "# loop through all models to get the mean cross validated score\n",
        "for name, model in models:\n",
        "    scoring = \"recall\"\n",
        "    kfold = StratifiedKFold(\n",
        "        n_splits=5, shuffle=True, random_state=1\n",
        "    )  # Setting number of splits equal to 5\n",
        "    cv_result = cross_val_score(\n",
        "        estimator=model, X=X_train, y=y_train, scoring=scoring, cv=kfold\n",
        "    )\n",
        "    results.append(cv_result)\n",
        "    names.append(name)\n",
        "    print(\"{}: {}\".format(name, cv_result.mean() * 100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M2uYjtlVGWrc"
      },
      "source": [
        "# Plotting boxplots for CV scores of all models defined above\n",
        "fig = plt.figure(figsize=(10, 7))\n",
        "\n",
        "fig.suptitle(\"Algorithm Comparison\")\n",
        "ax = fig.add_subplot(111)\n",
        "\n",
        "plt.boxplot(results)\n",
        "ax.set_xticklabels(names)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48oS1sdHGWre"
      },
      "source": [
        "- We can see that AdaBoost is giving the highest cross validated recall followed by XGBoost\n",
        "- The boxplot shows that the performance of both the models is consistent with just one outlier for AdaBoost.\n",
        "- We will tune both models - AdaBoost and XGBoost and see if the performance improves. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sENQdtrgGWrg"
      },
      "source": [
        "## Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNDzAlueGWrh"
      },
      "source": [
        "**We will use pipelines with StandardScaler and  AdaBoost model and tune the model using GridSearchCV and RandomizedSearchCV. We will also compare the performance and time taken by these two methods - grid search and randomized search.**\n",
        "\n",
        "**We can also use make_pipeline function instead of Pipeline to create a pipeline.**\n",
        "\n",
        "**`make_pipeline`: This is a shorthand for the Pipeline constructor; it does not require, and does not permit, naming the estimators. Instead, their names will be set to the lowercase of their types automatically.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GbmPjVZ8GWrj"
      },
      "source": [
        "**Fist, let's create two functions to calculate different metrics and confusion matrix, so that we don't have to use the same code repeatedly for each model.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQOh3Ls7GWrl"
      },
      "source": [
        "##  Function to calculate different metric scores of the model - Accuracy, Recall and Precision\n",
        "def get_metrics_score(model, flag=True):\n",
        "    \"\"\"\n",
        "    model : classifier to predict values of X\n",
        "\n",
        "    \"\"\"\n",
        "    # defining an empty list to store train and test results\n",
        "    score_list = []\n",
        "\n",
        "    pred_train = model.predict(X_train)\n",
        "    pred_test = model.predict(X_test)\n",
        "\n",
        "    train_acc = model.score(X_train, y_train)\n",
        "    test_acc = model.score(X_test, y_test)\n",
        "\n",
        "    train_recall = metrics.recall_score(y_train, pred_train)\n",
        "    test_recall = metrics.recall_score(y_test, pred_test)\n",
        "\n",
        "    train_precision = metrics.precision_score(y_train, pred_train)\n",
        "    test_precision = metrics.precision_score(y_test, pred_test)\n",
        "\n",
        "    score_list.extend(\n",
        "        (\n",
        "            train_acc,\n",
        "            test_acc,\n",
        "            train_recall,\n",
        "            test_recall,\n",
        "            train_precision,\n",
        "            test_precision,\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # If the flag is set to True then only the following print statements will be dispayed. The default value is set to True.\n",
        "    if flag == True:\n",
        "        print(\"Accuracy on training set : \", model.score(X_train, y_train))\n",
        "        print(\"Accuracy on test set : \", model.score(X_test, y_test))\n",
        "        print(\"Recall on training set : \", metrics.recall_score(y_train, pred_train))\n",
        "        print(\"Recall on test set : \", metrics.recall_score(y_test, pred_test))\n",
        "        print(\n",
        "            \"Precision on training set : \", metrics.precision_score(y_train, pred_train)\n",
        "        )\n",
        "        print(\"Precision on test set : \", metrics.precision_score(y_test, pred_test))\n",
        "\n",
        "    return score_list  # returning the list with train and test scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xuHjtKLyGWrn"
      },
      "source": [
        "## Function to create confusion matrix\n",
        "def make_confusion_matrix(model, y_actual, labels=[1, 0]):\n",
        "    \"\"\"\n",
        "    model : classifier to predict values of X\n",
        "    y_actual : ground truth\n",
        "\n",
        "    \"\"\"\n",
        "    y_predict = model.predict(X_test)\n",
        "    cm = metrics.confusion_matrix(y_actual, y_predict, labels=[0, 1])\n",
        "    df_cm = pd.DataFrame(\n",
        "        cm,\n",
        "        index=[i for i in [\"Actual - No\", \"Actual - Yes\"]],\n",
        "        columns=[i for i in [\"Predicted - No\", \"Predicted - Yes\"]],\n",
        "    )\n",
        "    group_counts = [\"{0:0.0f}\".format(value) for value in cm.flatten()]\n",
        "    group_percentages = [\"{0:.2%}\".format(value) for value in cm.flatten() / np.sum(cm)]\n",
        "    labels = [f\"{v1}\\n{v2}\" for v1, v2 in zip(group_counts, group_percentages)]\n",
        "    labels = np.asarray(labels).reshape(2, 2)\n",
        "    plt.figure(figsize=(10, 7))\n",
        "    sns.heatmap(df_cm, annot=labels, fmt=\"\")\n",
        "    plt.ylabel(\"True label\")\n",
        "    plt.xlabel(\"Predicted label\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIs2CcBXGWrp"
      },
      "source": [
        "## AdaBoost"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IW7dOvliGWrr"
      },
      "source": [
        "### GridSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SUJ2GJchGWrs"
      },
      "source": [
        "%%time \n",
        "\n",
        "# Creating pipeline\n",
        "pipe = make_pipeline(StandardScaler(), AdaBoostClassifier(random_state=1))\n",
        "\n",
        "# Parameter grid to pass in GridSearchCV\n",
        "param_grid = {\n",
        "    \"adaboostclassifier__n_estimators\": np.arange(10, 110, 10),\n",
        "    \"adaboostclassifier__learning_rate\": [0.1, 0.01, 0.2, 0.05, 1],\n",
        "    \"adaboostclassifier__base_estimator\": [\n",
        "        DecisionTreeClassifier(max_depth=1, random_state=1),\n",
        "        DecisionTreeClassifier(max_depth=2, random_state=1),\n",
        "        DecisionTreeClassifier(max_depth=3, random_state=1),\n",
        "    ],\n",
        "}\n",
        "\n",
        "# Type of scoring used to compare parameter combinations\n",
        "scorer = metrics.make_scorer(metrics.recall_score)\n",
        "\n",
        "# Calling GridSearchCV\n",
        "grid_cv = GridSearchCV(estimator=pipe, param_grid=param_grid, scoring=scorer, cv=5)\n",
        "\n",
        "# Fitting parameters in GridSeachCV\n",
        "grid_cv.fit(X_train, y_train)\n",
        "\n",
        "print(\n",
        "    \"Best Parameters:{} \\nScore: {}\".format(grid_cv.best_params_, grid_cv.best_score_)\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lamrsdPbGWrv"
      },
      "source": [
        "# Creating new pipeline with best parameters\n",
        "abc_tuned1 = make_pipeline(\n",
        "    StandardScaler(),\n",
        "    AdaBoostClassifier(\n",
        "        base_estimator=DecisionTreeClassifier(max_depth=2, random_state=1),\n",
        "        n_estimators=100,\n",
        "        learning_rate=1,\n",
        "        random_state=1,\n",
        "    ),\n",
        ")\n",
        "\n",
        "# Fit the model on training data\n",
        "abc_tuned1.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cv6jUVlaGWrx"
      },
      "source": [
        "# Calculating different metrics\n",
        "get_metrics_score(abc_tuned1)\n",
        "\n",
        "# Creating confusion matrix\n",
        "make_confusion_matrix(abc_tuned1, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SvY4hDjGWrz"
      },
      "source": [
        "- The test recall has increased by ~11% as compare to cross validated recall\n",
        "- The tuned adaboost model is slightly overfitting the training data\n",
        "- The test recall is still less than 50% i.e. the model is not good at identifying potential customers who would take the offer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffAehbHuGWr1"
      },
      "source": [
        "### RandomizedSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KgOuUpI3GWr2"
      },
      "source": [
        "%%time\n",
        "\n",
        "# Creating pipeline\n",
        "pipe = make_pipeline(StandardScaler(), AdaBoostClassifier(random_state=1))\n",
        "\n",
        "# Parameter grid to pass in GridSearchCV\n",
        "param_grid = {\n",
        "    \"adaboostclassifier__n_estimators\": np.arange(10, 110, 10),\n",
        "    \"adaboostclassifier__learning_rate\": [0.1, 0.01, 0.2, 0.05, 1],\n",
        "    \"adaboostclassifier__base_estimator\": [\n",
        "        DecisionTreeClassifier(max_depth=1, random_state=1),\n",
        "        DecisionTreeClassifier(max_depth=2, random_state=1),\n",
        "        DecisionTreeClassifier(max_depth=3, random_state=1),\n",
        "    ],\n",
        "}\n",
        "# Type of scoring used to compare parameter combinations\n",
        "scorer = metrics.make_scorer(metrics.recall_score)\n",
        "\n",
        "#Calling RandomizedSearchCV\n",
        "abc_tuned2 = RandomizedSearchCV(estimator=pipe, param_distributions=param_grid, n_iter=50, scoring=scorer, cv=5, random_state=1)\n",
        "\n",
        "#Fitting parameters in RandomizedSearchCV\n",
        "abc_tuned2.fit(X_train,y_train)\n",
        "\n",
        "print(\"Best parameters are {} with CV score={}:\" .format(abc_tuned2.best_params_,abc_tuned2.best_score_))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHAfV9YyGWr5"
      },
      "source": [
        "- Grid search took significantly longer time than random search. This difference would further increase as the number of parameters increases but the parameters from random search are exactly the same as compared grid search.\n",
        "- This can happen by chance but it is not guaranteed to happen for each algorithm."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlqLfNj8GWr6"
      },
      "source": [
        "## XGBoost"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2wz--MmkGWr9"
      },
      "source": [
        "### GridSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rPiHX0ilGWr_"
      },
      "source": [
        "%%time \n",
        "\n",
        "#Creating pipeline\n",
        "pipe=make_pipeline(StandardScaler(), XGBClassifier(random_state=1,eval_metric='logloss'))\n",
        "\n",
        "#Parameter grid to pass in GridSearchCV\n",
        "param_grid={'xgbclassifier__n_estimators':np.arange(50,300,50),'xgbclassifier__scale_pos_weight':[0,1,2,5,10],\n",
        "            'xgbclassifier__learning_rate':[0.01,0.1,0.2,0.05], 'xgbclassifier__gamma':[0,1,3,5],\n",
        "            'xgbclassifier__subsample':[0.7,0.8,0.9,1]}\n",
        "\n",
        "# Type of scoring used to compare parameter combinations\n",
        "scorer = metrics.make_scorer(metrics.recall_score)\n",
        "\n",
        "#Calling GridSearchCV\n",
        "grid_cv = GridSearchCV(estimator=pipe, param_grid=param_grid, scoring=scorer, cv=5)\n",
        "\n",
        "#Fitting parameters in GridSeachCV\n",
        "grid_cv.fit(X_train,y_train)\n",
        "\n",
        "\n",
        "print(\"Best parameters are {} with CV score={}:\" .format(grid_cv.best_params_,grid_cv.best_score_))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ItpsE26PGWsC"
      },
      "source": [
        "# Creating new pipeline with best parameters\n",
        "xgb_tuned1 = make_pipeline(\n",
        "    StandardScaler(),\n",
        "    XGBClassifier(\n",
        "        random_state=1,\n",
        "        n_estimators=50,\n",
        "        scale_pos_weight=10,\n",
        "        subsample=0.9,\n",
        "        learning_rate=0.01,\n",
        "        gamma=5,\n",
        "        eval_metric='logloss',\n",
        "    ),\n",
        ")\n",
        "\n",
        "# Fit the model on training data\n",
        "xgb_tuned1.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "boS4rvXFGWsF"
      },
      "source": [
        "# Calculating different metrics\n",
        "get_metrics_score(xgb_tuned1)\n",
        "\n",
        "# Creating confusion matrix\n",
        "make_confusion_matrix(xgb_tuned1, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOKAG-8OGWsI"
      },
      "source": [
        "- The test recall has increased by ~40% as compared to the result from cross validation with default parameters.\n",
        "- The model is overfitting the training data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRijJWnrGWsJ"
      },
      "source": [
        "### RandomizedSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpS3WhOhGWsL"
      },
      "source": [
        "%%time \n",
        "\n",
        "#Creating pipeline\n",
        "pipe=make_pipeline(StandardScaler(),XGBClassifier(random_state=1,eval_metric='logloss', n_estimators = 50))\n",
        "\n",
        "#Parameter grid to pass in GridSearchCV\n",
        "param_grid={'xgbclassifier__n_estimators':np.arange(50,300,50),\n",
        "            'xgbclassifier__scale_pos_weight':[0,1,2,5,10],\n",
        "            'xgbclassifier__learning_rate':[0.01,0.1,0.2,0.05],\n",
        "            'xgbclassifier__gamma':[0,1,3,5],\n",
        "            'xgbclassifier__subsample':[0.7,0.8,0.9,1],\n",
        "           'xgbclassifier__max_depth':np.arange(1,10,1),\n",
        "            'xgbclassifier__reg_lambda':[0,1,2,5,10]}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Type of scoring used to compare parameter combinations\n",
        "scorer = metrics.make_scorer(metrics.recall_score)\n",
        "\n",
        "#Calling RandomizedSearchCV\n",
        "randomized_cv = RandomizedSearchCV(estimator=pipe, param_distributions=param_grid, n_iter=50, scoring=scorer, cv=5, random_state=1)\n",
        "\n",
        "#Fitting parameters in RandomizedSearchCV\n",
        "randomized_cv.fit(X_train,y_train)\n",
        "\n",
        "print(\"Best parameters are {} with CV score={}:\" .format(randomized_cv.best_params_,randomized_cv.best_score_))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iw1RMQ3gGWsP"
      },
      "source": [
        "# Creating new pipeline with best parameters\n",
        "xgb_tuned2 = Pipeline(\n",
        "    steps=[\n",
        "        (\"scaler\", StandardScaler()),\n",
        "        (\n",
        "            \"XGB\",\n",
        "            XGBClassifier(\n",
        "                random_state=1,\n",
        "                n_estimators=200,\n",
        "                scale_pos_weight=10,\n",
        "                gamma=1,\n",
        "                subsample=0.9,\n",
        "                learning_rate= 0.01,\n",
        "                eval_metric='logloss', max_depth = 2, reg_lambda = 2\n",
        "            ),\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "# Fit the model on training data\n",
        "xgb_tuned2.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFXtsC54GWsS"
      },
      "source": [
        "# Calculating different metrics\n",
        "get_metrics_score(xgb_tuned2)\n",
        "\n",
        "# Creating confusion matrix\n",
        "make_confusion_matrix(xgb_tuned2, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_bnCMPYwGWsU"
      },
      "source": [
        "- Random search is giving better results than Grid search.\n",
        "- The test recall has increased as compared to the test recall from grid search but the accuracy and precision has decreased.\n",
        "- The overfitting in the model has also decreased"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWE6u-vlGWsV"
      },
      "source": [
        "## Comparing all models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9jvgWf9GWsX"
      },
      "source": [
        "# defining list of models\n",
        "models = [abc_tuned1, abc_tuned2, xgb_tuned1, xgb_tuned2]\n",
        "\n",
        "# defining empty lists to add train and test results\n",
        "acc_train = []\n",
        "acc_test = []\n",
        "recall_train = []\n",
        "recall_test = []\n",
        "precision_train = []\n",
        "precision_test = []\n",
        "\n",
        "# looping through all the models to get the metrics score - Accuracy, Recall and Precision\n",
        "for model in models:\n",
        "\n",
        "    j = get_metrics_score(model, False)\n",
        "    acc_train.append(j[0])\n",
        "    acc_test.append(j[1])\n",
        "    recall_train.append(j[2])\n",
        "    recall_test.append(j[3])\n",
        "    precision_train.append(j[4])\n",
        "    precision_test.append(j[5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-3-oRZTGWsZ"
      },
      "source": [
        "comparison_frame = pd.DataFrame(\n",
        "    {\n",
        "        \"Model\": [\n",
        "            \"Decision Tree with GridSearchCV\",\n",
        "            \"Decision Tree with RandomizedSearchCV\",\n",
        "            \"XGBoost with GridSearchCV\",\n",
        "            \"XGBoost with RandomizedSearchCV\",\n",
        "    \n",
        "        ],\n",
        "        \"Train_Accuracy\": acc_train,\n",
        "        \"Test_Accuracy\": acc_test,\n",
        "        \"Train_Recall\": recall_train,\n",
        "        \"Test_Recall\": recall_test,\n",
        "        \"Train_Precision\": precision_train,\n",
        "        \"Test_Precision\": precision_test,\n",
        "    }\n",
        ")\n",
        "\n",
        "# Sorting models in decreasing order of test recall\n",
        "comparison_frame.sort_values(by=\"Test_Recall\", ascending=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmPVdEgWGWsc"
      },
      "source": [
        "- The xgboost model tuned using randomised search is giving the best test recall of 0.87 but it has the least train and test precision.\n",
        "- Let's see the feature importance from the tuned xgboost model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kp3UVMqFGWse"
      },
      "source": [
        "feature_names = X_train.columns\n",
        "importances = xgb_tuned2[1].feature_importances_\n",
        "indices = np.argsort(importances)\n",
        "\n",
        "plt.figure(figsize=(12, 12))\n",
        "plt.title(\"Feature Importances\")\n",
        "plt.barh(range(len(indices)), importances[indices], color=\"violet\", align=\"center\")\n",
        "plt.yticks(range(len(indices)), [feature_names[i] for i in indices])\n",
        "plt.xlabel(\"Relative Importance\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7cYyaX2GWsh"
      },
      "source": [
        "- Amount spent on gold products is the most important feature, followed by NumCatalogPurchases and Recency of the customer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQZ0jtN9GWsj"
      },
      "source": [
        "## Business Recommendations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qfvsvWRGWsk"
      },
      "source": [
        "- Company should target customers who buy premium products - gold products or high quality wines - as these customers are able to spend more and are more likely to purchase the offer. The company should further launch premium offers for such customers. Such offers can also be extended to customers with higher income. \n",
        "- We observed in our analysis that ~64% of customers are married but single customers, including divorced and widowed, are more  equally or more likely to take the offer. Company should expand their customers by customizing offers to attract more single customers.\n",
        "- Customers who are frequent buyers, should be targeted more by the company and offer them added benefits.\n",
        "- Total amount spent has decreased over the years which shows that either our product qualities has declined or company lacks marketing strategies. Company should constantly improve their marketing strategies to address such issues.\n",
        "- Our analysis showed that ~99% customers had no complaints in the last two years which can be due to the lack of feedback options for customers. Company should create easy mechanisms to gather feedback from the customers and use it to identify major concerns, if any.\n",
        "- The number of web visits is an important feature and company should work on customizing their website to allow more traffic on the website. Company can improve the interface and provide easy check-in, check-out and delivery options."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hzjR_sxhGWsn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}